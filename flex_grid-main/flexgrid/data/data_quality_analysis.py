#!/usr/bin/env python3
"""
Analyse de Qualit√© des Donn√©es Historiques - EV2Gym
Analyse des donn√©es PV/Consommation/Grid avec d√©tection de pertes et nettoyage
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class DataQualityAnalyzer:
    def __init__(self, file_path):
        """Initialise l'analyseur avec le chemin du fichier de donn√©es"""
        self.file_path = file_path
        self.df = None
        self.quality_report = {}
        
    def load_data(self):
        """Charge les donn√©es depuis le fichier CSV"""
        try:
            print(f"üìä Chargement des donn√©es depuis: {self.file_path}")
            
            # Essayer diff√©rents encodages
            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
            
            for encoding in encodings:
                try:
                    self.df = pd.read_csv(self.file_path, encoding=encoding)
                    print(f"‚úÖ Donn√©es charg√©es avec succ√®s (encoding: {encoding})")
                    break
                except UnicodeDecodeError:
                    continue
            
            if self.df is None:
                raise Exception("Impossible de charger le fichier avec les encodages test√©s")
                
            print(f"üìà Dimensions des donn√©es: {self.df.shape}")
            print(f"üìÖ Colonnes disponibles: {list(self.df.columns)}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement: {str(e)}")
            return False
    
    def detect_datetime_column(self):
        """D√©tecte automatiquement la colonne de timestamp"""
        datetime_candidates = []
        
        for col in self.df.columns:
            # V√©rifier si le nom de colonne sugg√®re une date/heure
            if any(keyword in col.lower() for keyword in ['time', 'date', 'timestamp', 'datetime', 'heure', 'temps']):
                datetime_candidates.append(col)
                
        if datetime_candidates:
            return datetime_candidates[0]
        
        # Si pas trouv√© par nom, chercher par contenu
        for col in self.df.columns:
            try:
                pd.to_datetime(self.df[col].iloc[:10])
                return col
            except:
                continue
                
        return None
    
    def basic_quality_checks(self):
        """Effectue les v√©rifications de qualit√© de base"""
        print("\nüîç === V√âRIFICATIONS DE QUALIT√â DES DONN√âES ===")
        
        # 1. Informations g√©n√©rales
        self.quality_report['total_rows'] = len(self.df)
        self.quality_report['total_columns'] = len(self.df.columns)
        
        print(f"üìä Nombre total de lignes: {self.quality_report['total_rows']:,}")
        print(f"üìä Nombre total de colonnes: {self.quality_report['total_columns']}")
        
        # 2. Valeurs manquantes
        missing_data = self.df.isnull().sum()
        missing_percentage = (missing_data / len(self.df)) * 100
        
        self.quality_report['missing_data'] = missing_data.to_dict()
        self.quality_report['missing_percentage'] = missing_percentage.to_dict()
        
        print("\nüìâ VALEURS MANQUANTES:")
        for col in self.df.columns:
            if missing_data[col] > 0:
                print(f"  ‚Ä¢ {col}: {missing_data[col]:,} ({missing_percentage[col]:.2f}%)")
        
        # 3. Doublons
        duplicates = self.df.duplicated().sum()
        self.quality_report['duplicates'] = duplicates
        print(f"\nüîÑ Lignes dupliqu√©es: {duplicates:,}")
        
        # 4. Types de donn√©es
        print(f"\nüìã TYPES DE DONN√âES:")
        for col in self.df.columns:
            print(f"  ‚Ä¢ {col}: {self.df[col].dtype}")
        
        return self.quality_report
    
    def detect_data_loss(self):
        """D√©tecte les pertes de donn√©es et les gaps temporels"""
        print("\nüï≥Ô∏è === D√âTECTION DES PERTES DE DONN√âES ===")
        
        # D√©tecter la colonne de temps
        datetime_col = self.detect_datetime_column()
        
        if datetime_col is None:
            print("‚ùå Aucune colonne de timestamp d√©tect√©e")
            return
        
        print(f"üìÖ Colonne de timestamp d√©tect√©e: {datetime_col}")
        
        try:
            # Convertir en datetime
            self.df[datetime_col] = pd.to_datetime(self.df[datetime_col])
            self.df = self.df.sort_values(datetime_col)
            
            # Calculer les intervalles de temps
            time_diffs = self.df[datetime_col].diff()
            
            # D√©tecter l'intervalle normal (mode)
            normal_interval = time_diffs.mode().iloc[0] if len(time_diffs.mode()) > 0 else timedelta(minutes=15)
            
            print(f"‚è±Ô∏è Intervalle normal d√©tect√©: {normal_interval}")
            
            # D√©tecter les gaps (intervalles > 2x l'intervalle normal)
            threshold = normal_interval * 2
            gaps = time_diffs[time_diffs > threshold]
            
            self.quality_report['gaps'] = len(gaps)
            self.quality_report['normal_interval'] = str(normal_interval)
            
            print(f"üï≥Ô∏è Nombre de gaps d√©tect√©s: {len(gaps)}")
            
            if len(gaps) > 0:
                print("\nüìç GAPS D√âTECT√âS:")
                for idx, gap in gaps.items():
                    start_time = self.df.loc[idx-1, datetime_col] if idx > 0 else "D√©but"
                    end_time = self.df.loc[idx, datetime_col]
                    print(f"  ‚Ä¢ Gap de {gap} entre {start_time} et {end_time}")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse temporelle: {str(e)}")
    
    def energy_balance_analysis(self):
        """Analyse les bilans √©nerg√©tiques"""
        print("\n‚ö° === ANALYSE DES BILANS √âNERG√âTIQUES ===")
        
        # Identifier les colonnes d'√©nergie
        energy_columns = []
        for col in self.df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in ['pv', 'solar', 'production', 'consommation', 'consumption', 'grid', 'r√©seau', 'power', 'puissance', 'energy', 'energie']):
                energy_columns.append(col)
        
        print(f"üîã Colonnes d'√©nergie d√©tect√©es: {energy_columns}")
        
        if len(energy_columns) == 0:
            print("‚ùå Aucune colonne d'√©nergie d√©tect√©e")
            return
        
        # Statistiques descriptives
        print("\nüìä STATISTIQUES √âNERG√âTIQUES:")
        for col in energy_columns:
            if pd.api.types.is_numeric_dtype(self.df[col]):
                stats = self.df[col].describe()
                print(f"\n  ‚Ä¢ {col}:")
                print(f"    - Moyenne: {stats['mean']:.2f}")
                print(f"    - M√©diane: {stats['50%']:.2f}")
                print(f"    - Min: {stats['min']:.2f}")
                print(f"    - Max: {stats['max']:.2f}")
                print(f"    - √âcart-type: {stats['std']:.2f}")
                
                # D√©tecter les valeurs aberrantes
                Q1 = stats['25%']
                Q3 = stats['75%']
                IQR = Q3 - Q1
                outliers = self.df[(self.df[col] < Q1 - 1.5*IQR) | (self.df[col] > Q3 + 1.5*IQR)][col]
                print(f"    - Valeurs aberrantes: {len(outliers)} ({len(outliers)/len(self.df)*100:.2f}%)")
    
    def create_visualizations(self):
        """Cr√©e les visualisations des profils √©nerg√©tiques"""
        print("\nüìà === CR√âATION DES VISUALISATIONS ===")
        
        # D√©tecter les colonnes importantes
        datetime_col = self.detect_datetime_column()
        energy_columns = []
        
        for col in self.df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in ['pv', 'solar', 'production', 'consommation', 'consumption', 'grid', 'r√©seau', 'power', 'puissance']):
                if pd.api.types.is_numeric_dtype(self.df[col]):
                    energy_columns.append(col)
        
        if datetime_col is None or len(energy_columns) == 0:
            print("‚ùå Donn√©es insuffisantes pour les visualisations")
            return
        
        # Convertir la colonne de temps
        self.df[datetime_col] = pd.to_datetime(self.df[datetime_col])
        
        # Cr√©er les graphiques
        fig = make_subplots(
            rows=len(energy_columns), cols=1,
            subplot_titles=energy_columns,
            shared_xaxes=True,
            vertical_spacing=0.05
        )
        
        colors = px.colors.qualitative.Set1
        
        for i, col in enumerate(energy_columns):
            fig.add_trace(
                go.Scatter(
                    x=self.df[datetime_col],
                    y=self.df[col],
                    mode='lines',
                    name=col,
                    line=dict(color=colors[i % len(colors)]),
                    showlegend=True
                ),
                row=i+1, col=1
            )
        
        fig.update_layout(
            title="üìä Profils √ânerg√©tiques - PV/Consommation/Grid",
            height=300 * len(energy_columns),
            showlegend=True
        )
        
        fig.update_xaxes(title_text="Temps", row=len(energy_columns), col=1)
        
        # Sauvegarder le graphique
        fig.write_html("energy_profiles_analysis.html")
        print("‚úÖ Graphique sauvegard√©: energy_profiles_analysis.html")
        
        return fig
    
    def clean_data(self):
        """Nettoie les donn√©es si n√©cessaire"""
        print("\nüßπ === NETTOYAGE DES DONN√âES ===")
        
        original_rows = len(self.df)
        
        # 1. Supprimer les doublons
        self.df = self.df.drop_duplicates()
        print(f"üîÑ Doublons supprim√©s: {original_rows - len(self.df)}")
        
        # 2. Traiter les valeurs manquantes pour les colonnes num√©riques
        numeric_columns = self.df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            missing_count = self.df[col].isnull().sum()
            if missing_count > 0:
                # Interpolation lin√©aire pour les donn√©es temporelles
                self.df[col] = self.df[col].interpolate(method='linear')
                print(f"üìà {col}: {missing_count} valeurs interpol√©es")
        
        # 3. Sauvegarder les donn√©es nettoy√©es
        cleaned_filename = "cleaned_historical_data.csv"
        self.df.to_csv(cleaned_filename, index=False)
        print(f"‚úÖ Donn√©es nettoy√©es sauvegard√©es: {cleaned_filename}")
        
        return self.df
    
    def generate_report(self):
        """G√©n√®re un rapport complet de qualit√©"""
        print("\nüìã === RAPPORT DE QUALIT√â COMPLET ===")
        
        report = f"""
        üîç RAPPORT D'ANALYSE DE QUALIT√â DES DONN√âES
        ==========================================
        
        üìä INFORMATIONS G√âN√âRALES:
        - Nombre de lignes: {self.quality_report.get('total_rows', 'N/A'):,}
        - Nombre de colonnes: {self.quality_report.get('total_columns', 'N/A')}
        - Doublons d√©tect√©s: {self.quality_report.get('duplicates', 'N/A'):,}
        - Gaps temporels: {self.quality_report.get('gaps', 'N/A')}
        
        üìâ VALEURS MANQUANTES:
        """
        
        if 'missing_data' in self.quality_report:
            for col, missing in self.quality_report['missing_data'].items():
                if missing > 0:
                    percentage = self.quality_report['missing_percentage'][col]
                    report += f"        - {col}: {missing:,} ({percentage:.2f}%)\n"
        
        report += f"""
        ‚è±Ô∏è ANALYSE TEMPORELLE:
        - Intervalle normal: {self.quality_report.get('normal_interval', 'N/A')}
        
        ‚úÖ RECOMMANDATIONS:
        - V√©rifier les gaps temporels identifi√©s
        - Valider les valeurs aberrantes d√©tect√©es
        - Consid√©rer l'interpolation pour les valeurs manquantes
        - Surveiller la coh√©rence des bilans √©nerg√©tiques
        """
        
        # Sauvegarder le rapport
        with open("data_quality_report.txt", "w", encoding="utf-8") as f:
            f.write(report)
        
        print(report)
        print("‚úÖ Rapport sauvegard√©: data_quality_report.txt")

def main():
    """Fonction principale d'analyse"""
    file_path = r"C:\Users\MODERN\Downloads\hostorical data demosten.csv"
    
    print("üöÄ === ANALYSEUR DE QUALIT√â DES DONN√âES EV2GYM ===")
    print(f"üìÅ Fichier √† analyser: {file_path}")
    
    # Initialiser l'analyseur
    analyzer = DataQualityAnalyzer(file_path)
    
    # Charger les donn√©es
    if not analyzer.load_data():
        return
    
    # Effectuer les analyses
    analyzer.basic_quality_checks()
    analyzer.detect_data_loss()
    analyzer.energy_balance_analysis()
    analyzer.create_visualizations()
    analyzer.clean_data()
    analyzer.generate_report()
    
    print("\n‚úÖ === ANALYSE TERMIN√âE ===")
    print("üìÅ Fichiers g√©n√©r√©s:")
    print("  ‚Ä¢ energy_profiles_analysis.html - Visualisations interactives")
    print("  ‚Ä¢ cleaned_historical_data.csv - Donn√©es nettoy√©es")
    print("  ‚Ä¢ data_quality_report.txt - Rapport complet")

if __name__ == "__main__":
    main()
